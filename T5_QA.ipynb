{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa3c4149d75740378cc39d3019005874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9c35f9dc5064dc88b077f0e13a32e12",
              "IPY_MODEL_f3a3aab01a3745279f434b6b1f1b79cf",
              "IPY_MODEL_bff00d8951664e3a8bee67e43793bb3b"
            ],
            "layout": "IPY_MODEL_37f2f59dd6324cff9d532dbd946ef78a"
          }
        },
        "f9c35f9dc5064dc88b077f0e13a32e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3d48fd1f58a4ae9853a9a5264e4087c",
            "placeholder": "​",
            "style": "IPY_MODEL_4fa13f5245714a6fbb6b591cf395a0b0",
            "value": "Epoch 1: 100%"
          }
        },
        "f3a3aab01a3745279f434b6b1f1b79cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e779d23e3a2d4b69a3dd4ceb553c7e82",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ae83b9bb8784ae98dbc6bd69bad3a0c",
            "value": 250
          }
        },
        "bff00d8951664e3a8bee67e43793bb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba40e9c7430d418bb3050291a1acfa18",
            "placeholder": "​",
            "style": "IPY_MODEL_651f37a67aa44d03bdb1c90e2f68a808",
            "value": " 250/250 [32:19&lt;00:00,  7.09s/batch]"
          }
        },
        "37f2f59dd6324cff9d532dbd946ef78a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3d48fd1f58a4ae9853a9a5264e4087c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fa13f5245714a6fbb6b591cf395a0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e779d23e3a2d4b69a3dd4ceb553c7e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ae83b9bb8784ae98dbc6bd69bad3a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba40e9c7430d418bb3050291a1acfa18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "651f37a67aa44d03bdb1c90e2f68a808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0322a9368394215b018af2390a68244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11146fd43f9b4e7093996cb36cf3ecd4",
              "IPY_MODEL_14c539b0393149f68a9689d92e6e40d2",
              "IPY_MODEL_9600872d3cd3472a8d0412ac9950918c"
            ],
            "layout": "IPY_MODEL_3d218ec466984ff281d39f19d2e1bbf4"
          }
        },
        "11146fd43f9b4e7093996cb36cf3ecd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6796c53ab5048ac811764b2a6c0fe8a",
            "placeholder": "​",
            "style": "IPY_MODEL_508cb36a63a3444f893a1264cc50c247",
            "value": "Epoch 1: 100%"
          }
        },
        "14c539b0393149f68a9689d92e6e40d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f3037c10dc419bb70247ee36467dd2",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63ee418b8191466e863dc69c8b3b8563",
            "value": 250
          }
        },
        "9600872d3cd3472a8d0412ac9950918c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29308ce24d6b4856b4ab7195253333c7",
            "placeholder": "​",
            "style": "IPY_MODEL_917a495278534d3fb647ad1489b811fa",
            "value": " 250/250 [32:45&lt;00:00,  6.82s/batch]"
          }
        },
        "3d218ec466984ff281d39f19d2e1bbf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6796c53ab5048ac811764b2a6c0fe8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "508cb36a63a3444f893a1264cc50c247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57f3037c10dc419bb70247ee36467dd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ee418b8191466e863dc69c8b3b8563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29308ce24d6b4856b4ab7195253333c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917a495278534d3fb647ad1489b811fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0947ea3f8dbd45a3a199b4c11735fd8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd054c132e744123a2cebf38cb5b5864",
              "IPY_MODEL_23a8d916335946fab005c6064095ce13",
              "IPY_MODEL_5280b7da93e347c4a3489626f42ae626"
            ],
            "layout": "IPY_MODEL_58a44661887e4444a0d6942948d82f7f"
          }
        },
        "cd054c132e744123a2cebf38cb5b5864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d9109b736944ee8e7e602b2f2a1020",
            "placeholder": "​",
            "style": "IPY_MODEL_f07661515d714712a75fc4115ef09355",
            "value": "Generating Predictions: 100%"
          }
        },
        "23a8d916335946fab005c6064095ce13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30f0b1105ce445208692c81aa8ba8a40",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d19b874940e047339deed9ef89bb14ab",
            "value": 20
          }
        },
        "5280b7da93e347c4a3489626f42ae626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38ef8b2ec29946c7a52f05bb6638da16",
            "placeholder": "​",
            "style": "IPY_MODEL_6e9e7125df654c7a8bb52165477e7645",
            "value": " 20/20 [00:31&lt;00:00,  1.02it/s]"
          }
        },
        "58a44661887e4444a0d6942948d82f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d9109b736944ee8e7e602b2f2a1020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07661515d714712a75fc4115ef09355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f0b1105ce445208692c81aa8ba8a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19b874940e047339deed9ef89bb14ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38ef8b2ec29946c7a52f05bb6638da16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e9e7125df654c7a8bb52165477e7645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c1e220454374430850f0c44f321d737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9298a6617c4b4f1bb6b142ef348d38b8",
              "IPY_MODEL_60ff6fe19573494bb1f7e4fdaf93792f",
              "IPY_MODEL_78541a7571dd46838a8435765a849432"
            ],
            "layout": "IPY_MODEL_fc3fa75420b7444f8c739586fc3fc535"
          }
        },
        "9298a6617c4b4f1bb6b142ef348d38b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79856fa86dbe4574b66f4c877d333db2",
            "placeholder": "​",
            "style": "IPY_MODEL_3c5c3c6f653146a4b7c1ddc4da0a7d6d",
            "value": "Generating Predictions: 100%"
          }
        },
        "60ff6fe19573494bb1f7e4fdaf93792f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a84c99368cb47dca7371164c2cfd71f",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f3cfa6be88046fd9039e11e4e8a6000",
            "value": 20
          }
        },
        "78541a7571dd46838a8435765a849432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c064c0e4d066439caadb36b8090bf3fa",
            "placeholder": "​",
            "style": "IPY_MODEL_00ed7539c5944527889aeebf79543613",
            "value": " 20/20 [00:25&lt;00:00,  1.10s/it]"
          }
        },
        "fc3fa75420b7444f8c739586fc3fc535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79856fa86dbe4574b66f4c877d333db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c5c3c6f653146a4b7c1ddc4da0a7d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a84c99368cb47dca7371164c2cfd71f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3cfa6be88046fd9039e11e4e8a6000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c064c0e4d066439caadb36b8090bf3fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00ed7539c5944527889aeebf79543613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjGd8hddL3MH",
        "outputId": "913da95b-5529-4bba-915f-665c2b48733c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#T5-model without Type#"
      ],
      "metadata": {
        "id": "zo7rJv4en3sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Load the CSV file from Google Drive\n",
        "csv_file_path = '/content/drive/MyDrive/questions_responses.csv'\n",
        "df = pd.read_csv(csv_file_path, nrows=1000)  # Limit to first 1000 rows\n",
        "\n",
        "# Define a custom dataset class for question answering\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples.iloc[idx]\n",
        "        question = example[\"question\"]\n",
        "        response = example[\"response\"]\n",
        "        input_text = \"question: {} response: {}\".format(question, response)\n",
        "        target_text = response\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        target_ids = self.tokenizer.encode(target_text, return_tensors=\"pt\", max_length=32, truncation=True)\n",
        "        return {\"input_ids\": input_ids, \"labels\": target_ids}\n",
        "\n",
        "# Prepare the dataset for training\n",
        "train_dataset = QADataset(df, tokenizer)\n",
        "\n",
        "# Define the collate function to pad sequences dynamically within each batch\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
        "    labels = [item['labels'].squeeze(0) for item in batch]\n",
        "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    return {\"input_ids\": input_ids_padded, \"labels\": labels_padded}\n",
        "\n",
        "# Prepare the data loader with padding\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average loss for epoch {epoch + 1}: {average_loss}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./t5_qa_finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "fa3c4149d75740378cc39d3019005874",
            "f9c35f9dc5064dc88b077f0e13a32e12",
            "f3a3aab01a3745279f434b6b1f1b79cf",
            "bff00d8951664e3a8bee67e43793bb3b",
            "37f2f59dd6324cff9d532dbd946ef78a",
            "f3d48fd1f58a4ae9853a9a5264e4087c",
            "4fa13f5245714a6fbb6b591cf395a0b0",
            "e779d23e3a2d4b69a3dd4ceb553c7e82",
            "0ae83b9bb8784ae98dbc6bd69bad3a0c",
            "ba40e9c7430d418bb3050291a1acfa18",
            "651f37a67aa44d03bdb1c90e2f68a808"
          ]
        },
        "id": "erGltKwCMTmu",
        "outputId": "d4f463d2-6141-4c7c-c701-f205e51574b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/250 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa3c4149d75740378cc39d3019005874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss for epoch 1: 1.3478613460063935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  load model  t5_qa_finetuned\n",
        "\n",
        "model_path = \"./t5_qa_finetuned\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "9InNT9n29-vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MT5PoiigGKqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the model t5_qa_finetuned  to drive to folder T5_models\n",
        "\n",
        "!cp -r ./t5_qa_finetuned /content/drive/MyDrive/T5_models/t5_qa_finetuned\n"
      ],
      "metadata": {
        "id": "4tib2A2RGLc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#T5-model with Type#"
      ],
      "metadata": {
        "id": "A7O6sul4oHPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Load the CSV file from Google Drive\n",
        "csv_file_path = '/content/drive/MyDrive/questions_responses.csv'\n",
        "df = pd.read_csv(csv_file_path, nrows=1000)  # Limit to first 1000 rows\n",
        "\n",
        "# Define a custom dataset class for question answering\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples.iloc[idx]\n",
        "        question = example[\"question\"]\n",
        "        response = example[\"response\"]\n",
        "        question_type = example[\"type\"]\n",
        "        context = \"question: {} response: {} type: {}\".format(question, response, question_type)\n",
        "        input_text = \"{} type: {}\".format(context, example[\"type\"])\n",
        "        target_text = response\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        target_ids = self.tokenizer.encode(target_text, return_tensors=\"pt\", max_length=32, truncation=True)\n",
        "        return {\"input_ids\": input_ids, \"labels\": target_ids}\n",
        "\n",
        "# Prepare the dataset for training\n",
        "train_dataset = QADataset(df, tokenizer)\n",
        "\n",
        "# Define the collate function to pad sequences dynamically within each batch\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
        "    labels = [item['labels'].squeeze(0) for item in batch]\n",
        "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    return {\"input_ids\": input_ids_padded, \"labels\": labels_padded}\n",
        "\n",
        "# Prepare the data loader with padding\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Average loss for epoch {epoch + 1}: {average_loss}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./t5_qa_withType_finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "f0322a9368394215b018af2390a68244",
            "11146fd43f9b4e7093996cb36cf3ecd4",
            "14c539b0393149f68a9689d92e6e40d2",
            "9600872d3cd3472a8d0412ac9950918c",
            "3d218ec466984ff281d39f19d2e1bbf4",
            "c6796c53ab5048ac811764b2a6c0fe8a",
            "508cb36a63a3444f893a1264cc50c247",
            "57f3037c10dc419bb70247ee36467dd2",
            "63ee418b8191466e863dc69c8b3b8563",
            "29308ce24d6b4856b4ab7195253333c7",
            "917a495278534d3fb647ad1489b811fa"
          ]
        },
        "id": "ytSsDr8egU_R",
        "outputId": "4095f723-8f0b-4001-887a-b034a68126e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0322a9368394215b018af2390a68244",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1:   0%|          | 0/250 [00:00<?, ?batch/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss for epoch 1: 1.2973367706537247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the model t5_qa_withType_finetuned to drive to folder T5_models\n",
        "\n",
        "!cp -r ./t5_qa_withType_finetuned /content/drive/MyDrive/T5_models/t5_qa_withType_finetuned\n"
      ],
      "metadata": {
        "id": "7Rjp7QjeG8yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  load model t5_qa_withType_finetuned\n",
        "\n",
        "model_path = \"./t5_qa_withType_finetuned\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "relrnNVq-S3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation By Rough"
      ],
      "metadata": {
        "id": "gmSlGBy6RoMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dIXUhB4Slfb",
        "outputId": "67021f6e-5880-4971-acdc-97e251b85302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP1VSB40S-gC",
        "outputId": "9721211d-302f-4e0d-e56c-b0888732ac4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVJGeK3tXxJN",
        "outputId": "bce62c60-9889-4d90-ebd1-66b0b5be4bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=fd93df9941ded6d2852265bf530e0ceaf4b5f4431a8f82a35e1caa8ee15f2ecc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model without Question Type"
      ],
      "metadata": {
        "id": "CEvIiebZwX4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from evaluate import load\n",
        "from rouge_score import rouge_scorer\n",
        "# Load your trained T5 model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/NLP_QA/t5_qa_finetuned\"\n",
        "#tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/NLP_QA/questions_responses.csv'\n",
        "df = pd.read_csv(csv_file_path, skiprows=range(1, 1000), nrows=20)  # Load 200 rows after the 10000th row\n",
        "\n",
        "# Define a custom dataset class for question answering\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples.iloc[idx]\n",
        "        question = example[\"question\"]\n",
        "        response = example[\"response\"]\n",
        "        input_text = \"question: {} response: {}\".format(question, response)\n",
        "        target_text = response\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        target_ids = self.tokenizer.encode(target_text, return_tensors=\"pt\", max_length=32, truncation=True)\n",
        "        return {\"input_ids\": input_ids, \"labels\": target_ids, \"response\": response,\"question\": question}\n",
        "\n",
        "# Prepare the dataset for evaluation\n",
        "eval_dataset = QADataset(df, tokenizer)\n",
        "\n",
        "# Define a function to generate responses\n",
        "def generate_responses(model, tokenizer, questions):\n",
        "    generated_responses = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for question in tqdm(questions, desc=\"Generating Predictions\"):\n",
        "        input_text = \"question: {}\".format(question)\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "        output_ids = model.generate(input_ids=input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
        "        generated_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        generated_responses.append(generated_response)\n",
        "\n",
        "    return generated_responses\n",
        "\n",
        "# Generate responses for evaluation\n",
        "generated_responses = generate_responses(model, tokenizer, df['question'])\n",
        "\n",
        "# Load the ROUGE scorer\n",
        "#ROUGE-1, for instance, looks at individual words or unigrams, while ROUGE-2 considers pairs of words or bigrams\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Compute the ROUGE scores\n",
        "references = df['response'].tolist()\n",
        "\n",
        "\n",
        "for i in range(len(eval_dataset)):\n",
        "    print(\"Question\", i ,\":\",eval_dataset[i][\"question\"])\n",
        "    print(\"Response\",i,\":\",eval_dataset[i][\"response\"])\n",
        "    rouge_scores = scorer.score(generated_responses[i], eval_dataset[i][\"response\"])\n",
        "    print(\"Generated Response: \",generated_responses[i])\n",
        "    print(\"ROUGE scores:\", rouge_scores)\n",
        "    print(\"-----------------------------------------\")\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(rouge_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0947ea3f8dbd45a3a199b4c11735fd8c",
            "cd054c132e744123a2cebf38cb5b5864",
            "23a8d916335946fab005c6064095ce13",
            "5280b7da93e347c4a3489626f42ae626",
            "58a44661887e4444a0d6942948d82f7f",
            "c9d9109b736944ee8e7e602b2f2a1020",
            "f07661515d714712a75fc4115ef09355",
            "30f0b1105ce445208692c81aa8ba8a40",
            "d19b874940e047339deed9ef89bb14ab",
            "38ef8b2ec29946c7a52f05bb6638da16",
            "6e9e7125df654c7a8bb52165477e7645"
          ]
        },
        "id": "z-0XECX0WYLn",
        "outputId": "0d684307-247d-4a93-ef7c-16bec651d68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Predictions:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0947ea3f8dbd45a3a199b4c11735fd8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 0 : Translate the following sentence to German:\n",
            "This is why we are critical of the proposals put forward by the European Commission and supported by the Council to use the 'Barcelona Process' to undertake, in the framework of this process of association agreements with these countries and of the creation of a 'free trade zone', the liberalisation of services and of agriculture and, generally, the neoliberal guidelines laid down at the WTO Conference in Doha.\n",
            "\n",
            "German:\n",
            "Response 0 : Dies ist der Grund, warum wir die von der Europäischen Kommission vorgelegten und vom Rat unterstützten Vorschläge kritisch sehen, im Rahmen des \"Barcelona-Prozesses\" im Kontext der Assoziierungsabkommen mit diesen Ländern und der Schaffung einer \"Freihandelszone\" die Liberalisierung von Dienstleistungen und Landwirtschaft und im Allgemeinen, die neoliberalen Leitlinien, die auf der WTO-Konferenz in Doha festgelegt wurden, umzusetzen.\n",
            "\n",
            "Step-by-step translation and justification:\n",
            "1. \"This is why we are critical of the proposals\" = \"Dies ist der Grund, warum wir die ... Vorschläge kritisch sehen\"\n",
            "2. \"put forward by the European Commission\" = \"von der Europäischen Kommission vorgelegten\"\n",
            "3. \"and supported by the Council\" = \"und vom Rat unterstützten\"\n",
            "4. \"to use the 'Barcelona Process'\" = \"im Rahmen des 'Barcelona-Prozesses'\"\n",
            "5. \"to undertake, in the framework of this process\" = \"im Kontext der\"\n",
            "6. \"of association agreements with these countries\" = \"Assoziierungsabkommen mit diesen Ländern\"\n",
            "7. \"and of the creation of a 'free trade zone'\" = \"und der Schaffung einer 'Freihandelszone'\"\n",
            "8. \"the liberalisation of services and of agriculture\" = \"die Liberalisierung von Dienstleistungen und Landwirtschaft\"\n",
            "9. \"and, generally\" = \"und im Allgemeinen\"\n",
            "10. \"the neoliberal guidelines laid down at the WTO Conference in Doha\" = \"die neoliberalen Leitlinien, die auf der WTO-Konferenz in Doha festgelegt wurden\"\n",
            "Generated Response:  German:\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
            "-----------------------------------------\n",
            "Question 1 : For this chain-of-thought reasoning and answer, what was the question?\n",
            "If a mother and her child are hiking the mother is not running through the woods.\n",
            " A: no\n",
            "Response 1 : The question could be: \"Is the mother running through the woods while hiking with her child?\"\n",
            "\n",
            "My thought process to arrive at the answer \"no\" is as follows:\n",
            "\n",
            "1. The given statement says that the mother and her child are hiking.\n",
            "2. It also states that the mother is not running through the woods.\n",
            "3. Hiking typically involves walking at a moderate pace, not running.\n",
            "4. Since the mother is hiking with her child and not running through the woods, the answer to the question is \"no\".\n",
            "Generated Response:  if a mother and her child are hiking\n",
            "ROUGE scores: {'rouge1': Score(precision=0.08045977011494253, recall=0.875, fmeasure=0.14736842105263157), 'rouge2': Score(precision=0.05813953488372093, recall=0.7142857142857143, fmeasure=0.10752688172043011), 'rougeL': Score(precision=0.06896551724137931, recall=0.75, fmeasure=0.12631578947368421)}\n",
            "-----------------------------------------\n",
            "Question 2 : Please answer the following question: What key details about hermann oppenheim  can be extracted from the following bio?  Bio: hermann oppenheim -lrb- 1 january 1858 -- 5 may 1919 -rrb- was one of the leading neurologists in germany . he studied medicine at the universities of berlin , göttingen and bonn . he started his career at the charité-hospital in berlin as an assistant to karl westphal -lrb- 1833 -- 1890 -rrb- . in 1891 oppenheim opened a successful private hospital in berlin . in 1894 , oppenheim was the author of a textbook on nervous diseases titled `` lehrbuch der nervenkrankheiten für Ärzte und studierende '' , a book that soon became a standard in his profession . it was published in several editions and languages , and is considered one of the best textbooks on neurology ever written . he also published significant works on tabes dorsalis , alcoholism , anterior poliomyelitis , syphilis , multiple sclerosis and traumatic neurosis . in the field of physiology , he published articles on metabolism of urea with the aid of nathan zuntz -lrb- 1847 -- 1920 -rrb- . in 1889 he published a treatise on traumatic neuroses that was harshly criticized by eminent physicians that included jean-martin charcot -lrb- 1825 -- 1893 -rrb- and max nonne -lrb- 1861 -- 1959 -rrb- ; the reason being due to oppenheim 's assertion that psychological trauma caused organic changes that perpetuated psychic neuroses . his expertise involving brain disease led directly to the first successful removal of a brain tumor , an operation that was performed by a physician named r. köhler . with surgeon fedor krause -lrb- 1857 -- 1937 -rrb- , he reported the first successful removal of a pineal tumor . he coined the term '' `` dystonia musculorum deformans '' '' for a type of childhood torsion disease he described , a disorder that was later to became known as `` ziehen-oppenheim syndrome '' -lrb- named along with german psychiatrist theodor ziehen 1862 -- 1950 -rrb- . also , another name for amyotonia congenita is `` oppenheim 's disease '' .\n",
            "Answer:\n",
            "Response 2 : Hermann Oppenheim (1 January 1858 – 5 May 1919) was a prominent German neurologist. He studied medicine at the Universities of Berlin, Göttingen, and Bonn and began his career at the Charité-Hospital in Berlin as an assistant to Karl Westphal (1833-1890). In 1891, Oppenheim opened a successful private hospital in Berlin.\n",
            "\n",
            "Oppenheim authored a seminal textbook on nervous diseases in 1894, titled \"Lehrbuch der Nervenkrankheiten für Ärzte und Studierende,\" which became a standard in the field of neurology. The book was published in multiple editions and languages and is considered one of the best neurology textbooks ever written. He contributed significantly to the understanding of various neurological disorders such as tabes dorsalis, alcoholism, anterior poliomyelitis, syphilis, multiple sclerosis, and traumatic neurosis.\n",
            "\n",
            "In the field of physiology, Oppenheim published articles on urea metabolism in collaboration with Nathan Zuntz (1847-1920). His 1889 treatise on traumatic neuroses was met with criticism from renowned physicians Jean-Martin Charcot (1825-1893) and Max Nonne (1861-1959), as he asserted that psychological trauma caused organic changes leading to psychic neuroses.\n",
            "\n",
            "Oppenheim's expertise in brain diseases contributed to the first successful removal of a brain tumor, an operation performed by a physician named R. Köhler. Along with surgeon Fedor Krause (1857-1937), he reported the first successful removal of a pineal tumor.\n",
            "\n",
            "He also coined the term \"dystonia musculorum deformans\" for a childhood torsion disease he described, which later became known as \"Ziehen-Oppenheim syndrome\" (named in conjunction with German psychiatrist Theodor Ziehen, 1862-1950). Additionally, amyotonia congenita is also known as \"Oppenheim's disease\" in his honor.\n",
            "Generated Response:  bio: hermann oppenheim -lrb- 1 january 1858 -- 5 may 1919 -rrb\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0299625468164794, recall=0.7272727272727273, fmeasure=0.05755395683453238), 'rouge2': Score(precision=0.022556390977443608, recall=0.6, fmeasure=0.04347826086956521), 'rougeL': Score(precision=0.0299625468164794, recall=0.7272727272727273, fmeasure=0.05755395683453238)}\n",
            "-----------------------------------------\n",
            "Question 3 : Based on this review, would the user recommend this product? === Review: PIG DESTROYER - Terrifyer-This is some dark, twisted and evil music... yet I can't help but love it! PxDx is a 3 piece grind band that fuses many influences and has a sound so thick that it would consume many 5 pieces in their entirety. The blister guitar work of Scott Hull drives the charge while the rhythmically precise drumming of Brian Harvey holds it together. J.R. Hayes, who has my vote for the craziest lyricist vocalist since Today is the Day's Steve Austin, is in charge on the mic, pushing chaotic extreme vocals to new levels with a voice which is brutal yet still (at times) audible.So what you ask does PxDx sound like you might ask?Devastation! Destruction! Insanity! Chaos! Violence! The Apocalypse!........For extreme metal fans only, this is a treat for the ears.Favorite Songs: Thumbsucker, Gravedancer, Carrion Fairy, Towering Flesh, and The Gentleman.-4.5 StarsIF YOU LIKED, AGREED OR APPRECIATED THIS. PLEASE CLICK YES FOR:\"Was this review helpful?\" Answer:\n",
            "Choose your answer from: i. No ii. Yes\n",
            "Answer:\n",
            "Response 3 : ii. Yes\n",
            "Generated Response:  \n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 4 : You will be given a definition of a task first, then some input of the task.\n",
            "You are given a sentence in Italian. Your job is to translate the Italian sentence into Arabic.\n",
            "\n",
            "EM: Di solito sono 20 anni, ma la proposta è abbastanza diretta, mi sembra di capire.\n",
            "Output:\n",
            "Response 4 : عادة ما يكون عمري 20 عامًا ، لكن الاقتراح مباشر بما فيه الكفاية ، يبدو لي أنني أفهم.\n",
            "\n",
            "In this task, the definition is to translate an Italian sentence into Arabic. Given the input sentence in Italian, I used the definitions for each word and knowledge of Arabic grammar rules to convert the input into an Arabic sentence. I followed the same order and structure as the original Italian sentence, making sure the translated sentence maintains the same meaning and readability.\n",
            "Generated Response:  EM: Di solito sono 20 anni, ma la proposta è abbastanza diretta, mi sembra\n",
            "ROUGE scores: {'rouge1': Score(precision=0.015625, recall=0.07692307692307693, fmeasure=0.025974025974025976), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.015625, recall=0.07692307692307693, fmeasure=0.025974025974025976)}\n",
            "-----------------------------------------\n",
            "Question 5 : Here is a dialogue:\n",
            "Sally: Hi sorry but I can't make it on Monday - my mum has been taken into hospital.\n",
            "Anna: Oh no! What happened?\n",
            "Sally: She collapsed at home - they are just doing checks at the moment, but don't want to leave her.\n",
            "Anna: Of course! Hope she is feeling better soon.\n",
            "Sally: Yes, me too\n",
            "Anna: Are you still coming in December?\n",
            "Sally: Yes, hopefully - we can do something Christmassy.\n",
            "Anna: Yes that would be good. Say \"hi\" to your mum for me.\n",
            "Sally: Thanks I will. Will let you know how she gets on\n",
            "Anna: Thanks xxx\n",
            "\n",
            "What were they talking about?\n",
            "Response 5 : Sally and Anna were talking about how Sally can't make it to a plan they had on Monday because her mom had to go to the hospital. Sally's mom collapsed at home, so she wants to stay with her. They hope she feels better soon. Then they discuss if Sally will still be able to visit in December, and they want to do something Christmassy together. Anna asks Sally to say \"hi\" to her mom and Sally promises to update Anna about her mom's condition.\n",
            "Generated Response:  \n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 6 : Who played Cardinal Wolsey in the 1971 film ‘Carry On Henry’?\n",
            "Response 6 : In the 1971 funny movie called \"Carry On Henry,\" there's a character called Cardinal Wolsey. A talented actor named Terry Scott played this role. He dressed up like a Cardinal and acted so people could laugh and have a good time while watching the movie.\n",
            "Generated Response:  Who\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
            "-----------------------------------------\n",
            "Question 7 : Data: Bolt (comicsCharacter), creator, Ernie Colón\n",
            "\n",
            "Sentence about the following data: \n",
            "Response 7 : Bolt, a comics character created by the talented artist Ernie Colón, has captivated the hearts of fans worldwide.\n",
            "\n",
            "Step-by-step justification:\n",
            "1. Identify the main subject: In this case, the main subject is \"Bolt,\" a comics character.\n",
            "2. Identify the secondary subject: The secondary subject is the creator, \"Ernie Colón.\"\n",
            "3. Form the initial sentence structure: [Bolt] is a [comics character] created by [Ernie Colón].\n",
            "4. Enhance the sentence with adjectives or additional information: To make the sentence more interesting, we can add adjectives like \"talented\" to describe the creator, and phrases such as \"captivated the hearts of\" to show the impact the character has had on fans.\n",
            "5. Finalize the sentence: The final sentence is, \"Bolt, a comics character created by the talented artist Ernie Colón, has captivated the hearts of fans worldwide.\"\n",
            "Generated Response:  Ernie Colón Sentence\n",
            "ROUGE scores: {'rouge1': Score(precision=0.02877697841726619, recall=1.0, fmeasure=0.055944055944055944), 'rouge2': Score(precision=0.014492753623188406, recall=0.6666666666666666, fmeasure=0.028368794326241138), 'rougeL': Score(precision=0.02877697841726619, recall=1.0, fmeasure=0.055944055944055944)}\n",
            "-----------------------------------------\n",
            "Question 8 : Q: Lee jumped back into the River after he got his beer.  Given the context: What does Lee need to do before this?\n",
            "The answer is:\n",
            "Response 8 : Before jumping back into the river, Lee needs to get out of the river, go to a place where he can obtain his beer (either a store, bar, or cooler), and then grab his beer.\n",
            "Generated Response:  The answer is:\n",
            "ROUGE scores: {'rouge1': Score(precision=0.02857142857142857, recall=0.3333333333333333, fmeasure=0.05263157894736842), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.02857142857142857, recall=0.3333333333333333, fmeasure=0.05263157894736842)}\n",
            "-----------------------------------------\n",
            "Question 9 : Даже при правильной их конфигурации опытный и настойчивый хакер способен перехватить эти данные.\n",
            "\n",
            "Translate to English\n",
            "Response 9 : \"Even with their correct configuration, an experienced and persistent hacker can intercept this data.\"\n",
            "\n",
            "Although modern security technologies and measures offer robust protection against digital threats, it is important to understand that no system can be completely invulnerable to attacks by skilled and determined hackers. In this statement, the author acknowledges this fact by stating that despite having the correct configuration for protecting data, experienced and persistent hackers may still find a way to intercept this information.\n",
            "\n",
            "This emphasizes the importance of continuous improvement in security practices, staying up-to-date with the latest advancements in the field, and being vigilant about potential security breaches. It is crucial for individuals and organizations dealing with sensitive information to be proactive in maintaining their data security by investing in the latest tools, conducting regular security audits, and training their staff to follow best practices.\n",
            "\n",
            "Additionally, one should consider implementing multiple layers of security (also known as a defense-in-depth strategy) that minimize the potential for a single point of failure and improve overall resilience against cyber attacks. This would include utilizing a combination of encryption, firewalls, intrusion detection systems, and other security measures to make it harder for attackers to gain unauthorized access to data and systems.\n",
            "\n",
            "Moreover, it is essential for organizations to have a well-defined incident response plan in place to efficiently and effectively deal with any security breaches that may occur. This plan would typically include steps such as identification, containment, eradication, recovery, and follow-up to ensure minimal impact on business operations and to prevent future incidents.\n",
            "\n",
            "In summary, it is crucial to recognize that even with the right security measures in place, experienced and persistent hackers can still potentially intercept data. Therefore, it is necessary for individuals and organizations to continuously update their security measures, implement multi-layered defenses, and be prepared to respond effectively to security incidents.\n",
            "Generated Response:  Translate to English English\n",
            "ROUGE scores: {'rouge1': Score(precision=0.003194888178913738, recall=0.25, fmeasure=0.006309148264984227), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.003194888178913738, recall=0.25, fmeasure=0.006309148264984227)}\n",
            "-----------------------------------------\n",
            "Question 10 : Read the following article and answer the question by choosing from the options.\n",
            "\n",
            "Of course , she is having pain but that is to be expected . It 's her right hand so she will be needing help with the basics and her insulin shots but I think we ' ve got that worked out between her kids and me . Her brother and sister - in - law left to go back to Indy yesterday . So , now we have peace and quiet again .\n",
            "\n",
            "What type of disease might the female subject have ?\n",
            "Choose your answer from:\n",
            "a). She has cancer .;\n",
            "b). She has diabetes .;\n",
            "c). None of the above choices .;\n",
            "d). She has chlamydia .;...A:\n",
            "Response 10 : b). She has diabetes.\n",
            "Generated Response:  She has cancer.; b). She has diabetes.; c). None of the above choices\n",
            "ROUGE scores: {'rouge1': Score(precision=1.0, recall=0.3076923076923077, fmeasure=0.47058823529411764), 'rouge2': Score(precision=1.0, recall=0.25, fmeasure=0.4), 'rougeL': Score(precision=1.0, recall=0.3076923076923077, fmeasure=0.47058823529411764)}\n",
            "-----------------------------------------\n",
            "Question 11 : Question: What type of details about james g. mitchell  can be gathered from the following bio?  Bio: james george `` jim '' mitchell -lrb- born 25 april 1943 -rrb- is a canadian computer scientist . he has worked on programming language design and implementation -lrb- fortran , mesa , euclid , c++ , java -rrb- , interactive programming systems , dynamic interpretation and compilation , document preparation systems , user interface design , distributed transactional file systems , and distributed , object-oriented operating systems . he has also worked on the design of hardware for computer graphics , high-level language execution , and audio input output .\n",
            "Answer:\n",
            "Response 11 : The bio provides several significant details about James G. Mitchell.\n",
            "\n",
            "1. Personal Information: James G. Mitchell, also referred to as \"Jim,\" was born on April 25, 1943. He is a Canadian, indicating his nationality. \n",
            "\n",
            "2. Career Field: Mitchell is a computer scientist, implying that his expertise lies in computing technology. It's an extensive field that envelops numerous areas, such as programming, operating systems, computer graphics, and more. His career has been centered around the innovation and evolution of this technology.\n",
            "\n",
            "3. Specific Expertise: He has worked on programming language design and implementation. The bio particularly mentions FORTRAN, Mesa, Euclid, C++, and Java. These are all coding languages that have different applications, suggesting a wide-ranging skill set and in-depth knowledge of programming.\n",
            "\n",
            "4. Interactive Programming Systems: The mention of interactive programming systems indicates his work with interfaces that allow direct communication between the user and the computer program, like developing tools or programs that provide immediate responses to user inputs.\n",
            "\n",
            "5. Dynamic Interpretation and Compilation: Mitchell also has knowledge of dynamic interpretation and compilation. This signifies his comprehension of how programs are executed, which is important for optimizing performance and ensuring that software runs correctly.\n",
            "\n",
            "6. Document Preparation Systems: His work on document preparation systems denotes his involvement in developing software for generating different types of documentation, which can include manuals, guides, and tutorials, among others.\n",
            "\n",
            "7. User Interface Design: He has experience in user interface design, the process of making interfaces in software or computerized devices with a focus on looks or style and on usability and efficient interaction and operation.\n",
            "\n",
            "8. Distributed Transactional File Systems: His expertise in distributed transactional file systems suggests that he understands distributed systems that manage file storage and implementation across multiple sites, nodes, or environments and can ensure that transactions are processed correctly and efficiently.\n",
            "\n",
            "9. Object-Oriented Operating Systems: His work on distributed, object-oriented operating systems shows his acumen for building and maintaining systems that manage computer resources and provide various services.\n",
            "\n",
            "10. Hardware Design: Mitchell's work in the design of hardware for computer graphics, high-level language execution, and audio input/output indicates his range of competences, from developing hardware that accelerates graphic processing, working on hardware that efficiently executes high-level programming language, and creating technology for better sound processing.\n",
            "\n",
            "Overall, the details in the bio depict James G. Mitchell as a well-rounded computer scientist with expertise in various areas of computing technology, from programming languages to hardware design.\n",
            "Generated Response:  Answer:\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
            "-----------------------------------------\n",
            "Question 12 : I can use this background: Greenhouse Effect: The solar energy reaching the surface of the Earth is concentrated in short wavelengths, which can easily penetrate the greenhouse gases, such as carbon dioxide and methane. The Earth, however, is cooler than the sun and it radiates its heat in the form of energy in the far infrared range. These longer wavelengths are partially absorbed by the greenhouse gases and some of the solar heat is returned to Earth. At a certain temperature these processes are in equilibrium and the surface temperature of the Earth is stable. However, if more greenhouse gases are put in the atmosphere the amount of trapped terrestrial radiation increases, leading to an increase in global temperature.  Now, I have a new situation: David was visiting the natural history museum. He noticed two charts that showed the climatic conditions of the earth in two time periods, time A and time B. Time A showed earth's overall climate in the sixteenth century when there were less greenhouse gases in the atmosphere. And time B showed earth's overall climate in the present century with more greenhouse gases in the atmosphere. David found some interesting differences between these two charts.  Answer this question please: Which period would see more trapped terrestrial radiation, time A or time B?\n",
            "Answer:\n",
            "Response 12 : Time B would see more trapped terrestrial radiation. This is because the present century (Time B) has a higher concentration of greenhouse gases in the atmosphere as compared to the sixteenth century (Time A). As a result, more terrestrial radiation is being absorbed and retained by the greenhouse gases during Time B, leading to an overall increase in global temperatures due to the enhanced greenhouse effect.\n",
            "Generated Response:  Answer:\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
            "-----------------------------------------\n",
            "Question 13 : Choose the next sentence.NEW DELHI, India (CNN) -- India's child-rights watchdog has sought a report from police investigating allegations by a tabloid that the father of a \"Slumdog Millionaire\" child star tried to sell her to an undercover reporter, the watchdog's leader told CNN. Rubina Ali has backed her father over newspaper allegations he offered her to an undercover reporter. \"We have sought a report from them and will take a decision after seeing it,\" said Shantha Sinha, who heads the National Commission for Protection of Child Rights. Meanwhile, authorities in Mumbai have recorded the statements of Rafiq Qureshi; his \"Slumdog\" daughter, Rubina Ali; and his former wife, Khurshida Begum, senior police inspector Prakash Salunke told CNN.\n",
            "\n",
            "\n",
            "\n",
            "OPTIONS:\n",
            "- CNN's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Father's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- India's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Indian's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Khurshida Begum's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Mumbai's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- NEW DELHI's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- National Commission for Protection of Child Rights's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Prakash Salunke's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Rafiq Qureshi's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Rubina Ali's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Shantha Sinha's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Slumdog's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Slumdog Millionaire's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- UK's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "\n",
            "\n",
            "Response 13 : - Rafiq Qureshi's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "\n",
            "All the other options are wrong because they mention entities (like CNN, India, Mumbai, etc.) that cannot have a former wife. The correct answer is about Rafiq Qureshi, the father of the \"Slumdog Millionaire\" child star, because the question is about him and the allegations against him.\n",
            "Generated Response:  \n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 14 : Question: what district is sacramento? I found the following answer on Google: Sacramento grew quickly thanks to the protection of Sutter's Fort , which was established by Sutter in 1839. Is that a correct answer? Yes or no.\n",
            "The answer to this question is:\n",
            "Response 14 : No, the information you found does not answer your question about the district of Sacramento. \n",
            "\n",
            "Sacramento is both the capital of California and the county seat of Sacramento County. It is located in California's 6th congressional district, represented by Congresswoman Doris Matsui in the U.S. House of Representatives. Additionally, it is part of California's 6th State Assembly district, represented by Assemblymember Kevin McCarty, and California's 6th State Senate district, represented by Senator Richard Pan. \n",
            "\n",
            "The information you found relates to the history of how Sacramento grew rapidly due to the protection provided by Sutter's Fort, which was established by John Sutter in 1839. Although this is an interesting historical fact, it does not pertain directly to the district of Sacramento.\n",
            "Generated Response:  Sutter's Fort\n",
            "ROUGE scores: {'rouge1': Score(precision=0.024, recall=1.0, fmeasure=0.046875), 'rouge2': Score(precision=0.016129032258064516, recall=1.0, fmeasure=0.031746031746031744), 'rougeL': Score(precision=0.024, recall=1.0, fmeasure=0.046875)}\n",
            "-----------------------------------------\n",
            "Question 15 : Большинство квартир расположены в местах с высокоразвитой инфраструктурой, куда включены магазины различной направленности, небольшие, уютные кафе - все просто создано для полноценной счастливой жизни!\n",
            "\n",
            "Could you please translate this to English?\n",
            "Response 15 : Most apartments are located in places with highly developed infrastructure, which includes different kinds of shops, small cozy cafes - everything is just made for a full, happy life!\n",
            "Generated Response:  олинство квартир\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 16 : Possible answers:\n",
            " 1). cuts the parchment paper in half and cuts pieces to carve a face into a face and put a smiley face on the face.\n",
            " 2). fills the cup with water then serves it with a spoon.\n",
            " 3). weld paper in two and starts to wrap the cake with the paper in a strainer to hold onto, ring the cookie up on the baking sheet.\n",
            " 4). brushes on oil to the parchment paper then sets it into a cooking tin.Choose from options above and answer: What most naturally follows?\n",
            "\n",
            "A person measures and cuts parchment paper using a cooking tin as a guide. The person measures ingredients in a glass bowl then adds them to an electric mixer to blend. the person\n",
            "Answer:\n",
            "Response 16 : 4). brushes on oil to the parchment paper then sets it into a cooking tin.\n",
            "\n",
            "Explanation: The person has already measured and prepared the parchment paper and ingredients. The most logical next step would be to prepare the parchment paper for baking by brushing it with oil and setting it into the cooking tin. \n",
            "\n",
            "Option 1 is wrong because it talks about carving a face, which doesn't relate to baking or parchment paper.\n",
            "Option 2 is incorrect because it involves serving water with a spoon, which doesn't follow the situation described.\n",
            "Option 3 is not suitable because it involves wrapping a cake in paper, while the situation describes preparing for baking, not handling an already baked cake.\n",
            "Generated Response:  fills the cup with water then serves it with a spoon. 3). weld paper in two and starts to wrap the cake with the paper\n",
            "ROUGE scores: {'rouge1': Score(precision=0.15966386554621848, recall=0.76, fmeasure=0.26388888888888884), 'rouge2': Score(precision=0.025423728813559324, recall=0.125, fmeasure=0.04225352112676057), 'rougeL': Score(precision=0.09243697478991597, recall=0.44, fmeasure=0.15277777777777776)}\n",
            "-----------------------------------------\n",
            "Question 17 : Please answer the following question: What is the answer for the question: What gender of the Perijá tapaculo have brown on the back of their legs and whitish on the front? from the following article ?  The Perijá tapaculo is a small bird, 10 to 12 cm (3.9 to 4.7 in) in length with an average mass of 17 to 18 grams (around 0.6 oz). The bill averages 6.8 millimetres (0.27 inches) long, 2.9 mm (0.11 in) wide, and 3.5 mm (0.14 in) high. The legs are about 21 mm (0.83 in) long. The Perijá tapaculo's tarsus averages 21.1 millimetres (0.83 in) long. The wings measure 57.4 mm (2.26 in) on average and the tail is about 40 mm (1.6 in) long with between 8 and 12 rectrices.The forehead, lores, crown, mantle, and scapular area are a neutral grey colour. There is a brown spot on the nape. The top of the tail is brown, and the bottom is faintly striped brown. The bird's back and rump are striped brown-sepia, and the throat, breast, and belly are grey-white. Its lower belly and flanks are tawny. The iris is dark brown. Male specimens are distinguished by having less sharp brown spots on their napes, and the bottom of their breasts are mixed with a pale buff colour. The legs are brown on the back and whitish on the front. Young birds have a yellowish appearance with striped brown flanks.The bird's plumage colouration is most similar to the pale-bellied tapaculo. The S. g. morenoi subspecies of the pale-bellied tapaculo can be differentiated from the Perijá tapaculo by its entirely brown back and nape, and its different calls. Juveniles of this subspecies have a dull ventral plumage, while the Perijá tapaculo has a more yellow plumage. It also resembles the Caracas tapaculo but has a duller ventral colour.\n",
            "A:\n",
            "Response 17 : After reading the article, I can determine that the answer to the question \"What gender of the Perijá tapaculo have brown on the back of their legs and whitish on the front?\" is male specimens. The relevant information can be found in the following sentence: \"Male specimens are distinguished by having less sharp brown spots on their napes, and the bottom of their breasts are mixed with a pale buff colour. The legs are brown on the back and whitish on the front.\"\n",
            "Generated Response:  The Perijá tapaculo is a small bird, 10 to 12 cm (3.9 to 4.7 in) in length\n",
            "ROUGE scores: {'rouge1': Score(precision=0.08433734939759036, recall=0.3684210526315789, fmeasure=0.1372549019607843), 'rouge2': Score(precision=0.024390243902439025, recall=0.1111111111111111, fmeasure=0.04), 'rougeL': Score(precision=0.060240963855421686, recall=0.2631578947368421, fmeasure=0.0980392156862745)}\n",
            "-----------------------------------------\n",
            "Question 18 : I have a new situation: Deep lake presents geothermal activity, while Shallow lake has mixing of saline water with fresh water. Deep lake had more instances of divers having problems because of thermoclines.  But I can use this background: Thermoclines, or sudden changes in temperature. Where the air temperature is higher than the water temperature, shallow water may be warmed by the air and the sunlight but deeper water remains cold resulting in a lowering of temperature as the diver descends. This temperature change may be concentrated over a small vertical interval, when it is called a thermocline.[3][4] Where cold, fresh water enters a warmer sea the fresh water may float over the denser saline water, so the temperature rises as the diver descends.[3] In lakes exposed to geothermal activity, the temperature of the deeper water may be warmer than the surface water. This will usually lead to convection currents.[3]  What is an answer for this question: Which lake has warmer water the bottom?\n",
            "A:\n",
            "Response 18 : Based on the given information and background, Deep Lake has warmer water at the bottom due to its geothermal activity. \n",
            "\n",
            "In the context of the two lakes, Deep Lake exhibits geothermal activity, which means it is influenced by the Earth's internal heat. This heat rises towards the surface and, in the case of Deep Lake, the temperature of the deeper water may be warmer than the surface water. This causes convection currents, as the warmer, less dense water rises and the cooler, denser water sinks, creating a continuous cycle. \n",
            "\n",
            "On the other hand, Shallow Lake has a mix of saline water with fresh water. While temperature variations can occur in this lake due to the different densities and solubilities of saline and fresh water, it is not mentioned that Shallow Lake has any source of geothermal activity like Deep Lake. This means that its deep water's temperatures will be influenced more by the surrounding environmental factors, and not by heat from within the Earth, which is the case in Deep Lake.\n",
            "\n",
            "In conclusion, Deep Lake has warmer water at the bottom due to the geothermal activity it experiences. This heat, originating from the Earth's interior, progressively warms the deeper waters of the lake and creates distinct thermoclines that divers experience when they descend through the water column. Meanwhile, Shallow Lake's water temperature is more influenced by the mixing of saline and fresh water, without a specific indication of geothermal activity making its deep water warmer compared to Deep Lake.\n",
            "Generated Response:  Deep lake presents geothermal activity,\n",
            "ROUGE scores: {'rouge1': Score(precision=0.015810276679841896, recall=0.8, fmeasure=0.031007751937984492), 'rouge2': Score(precision=0.007936507936507936, recall=0.5, fmeasure=0.015625), 'rougeL': Score(precision=0.015810276679841896, recall=0.8, fmeasure=0.031007751937984492)}\n",
            "-----------------------------------------\n",
            "Question 19 : Write an article that answers the following question: Who threw the longest touchdown pass of the game?\n",
            "Response 19 : Title: Reliving the Longest Touchdown Pass of the Game: A Moment of Glory\n",
            "\n",
            "Introduction\n",
            "\n",
            "In football, the touchdown pass is one of the most crucial and game-defining plays. Spectators' hearts race when a long touchdown pass is executed with precision and grace, leaving fans in awe and opponents in disbelief. In this article, we will uncover the player responsible for the longest touchdown pass of the game, describe the scenarious and events that led to the throw, and the significance of this play both in the context of the game and the player's career.\n",
            "\n",
            "Step 1: Research\n",
            "\n",
            "To answer the question and determine who threw the longest touchdown pass of the game, I will conduct thorough research using reliable sports websites, news articles, and real-time updates. I will gather accurate data about every touchdown pass that occurred in the game, compare the lengths and timing, and identify the longest throw.\n",
            "\n",
            "Step 2: Identify the key players and plays\n",
            "\n",
            "Once the research is complete, I will have identified the quarterback responsible for the longest touchdown pass of the game along with the recipient. I will also highlight other notable plays during the game, particularly those that contributed to the magnitude of the subsequent pass.\n",
            "\n",
            "Step 3: Describe the scenario and events leading to the throw\n",
            "\n",
            "In detailing the longest touchdown pass, I will include the timing, distance, and field position. Furthermore, I will discuss the strategies and decisions of both the offensive and defensive teams leading to that specific play. This information will give readers a comprehensive understanding of the factors at play before and during the game-defining throw.\n",
            "\n",
            "Step 4: Explain the significance of the longest touchdown pass\n",
            "\n",
            "In this section, I will discuss the implications of the longest touchdown pass on the outcome of the game, specifically how it impacted the teams' scores, morale, and strategy. Additionally, I will explore the play's influence on the players' career trajectories, including that of the quarterback, recipient, and other key participants involved in the play.\n",
            "\n",
            "Conclusion\n",
            "\n",
            "By recounting the exceptional athletic prowess that produced the longest touchdown pass of the game, this article aims to give readers an immersive experience of the game's exhilarating moments. Players' careers and legacies will be assessed based on the plays or contributions made towards the touchdown, providing an insider viewpoint on the forces at play during auspicious match-ups. Football fans and enthusiasts will relish the opportunity to delve into the intricate strategy, precise execution, and sheer athleticism that comes together in the unmatched spectacle of the sport's longest touchdown pass.\n",
            "Generated Response:  Who threw\n",
            "ROUGE scores: {'rouge1': Score(precision=0.004629629629629629, recall=1.0, fmeasure=0.009216589861751152), 'rouge2': Score(precision=0.002320185614849188, recall=1.0, fmeasure=0.004629629629629629), 'rougeL': Score(precision=0.004629629629629629, recall=1.0, fmeasure=0.009216589861751152)}\n",
            "-----------------------------------------\n",
            "{'rouge1': Score(precision=0.004629629629629629, recall=1.0, fmeasure=0.009216589861751152), 'rouge2': Score(precision=0.002320185614849188, recall=1.0, fmeasure=0.004629629629629629), 'rougeL': Score(precision=0.004629629629629629, recall=1.0, fmeasure=0.009216589861751152)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model with Question Type"
      ],
      "metadata": {
        "id": "OtP1nXDpwNL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from evaluate import load\n",
        "from rouge_score import rouge_scorer\n",
        "# Load your trained T5 model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/NLP_QA/t5_qa_withType_finetuned\"\n",
        "#tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/NLP_QA/questions_responses.csv'\n",
        "df = pd.read_csv(csv_file_path, skiprows=range(1, 1000), nrows=20)  # Load 200 rows after the 10000th row\n",
        "\n",
        "# Define a custom dataset class for question answering\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples.iloc[idx]\n",
        "        question = example[\"question\"]\n",
        "        response = example[\"response\"]\n",
        "        input_text = \"question: {} response: {}\".format(question, response)\n",
        "        target_text = response\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        target_ids = self.tokenizer.encode(target_text, return_tensors=\"pt\", max_length=32, truncation=True)\n",
        "        return {\"input_ids\": input_ids, \"labels\": target_ids, \"response\": response,\"question\": question}\n",
        "\n",
        "# Prepare the dataset for evaluation\n",
        "eval_dataset = QADataset(df, tokenizer)\n",
        "\n",
        "# Define a function to generate responses\n",
        "def generate_responses(model, tokenizer, questions):\n",
        "    generated_responses = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for question in tqdm(questions, desc=\"Generating Predictions\"):\n",
        "        input_text = \"question: {}\".format(question)\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "        output_ids = model.generate(input_ids=input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
        "        generated_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        generated_responses.append(generated_response)\n",
        "\n",
        "    return generated_responses\n",
        "\n",
        "# Generate responses for evaluation\n",
        "generated_responses = generate_responses(model, tokenizer, df['question'])\n",
        "\n",
        "# Load the ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Compute the ROUGE scores\n",
        "for i in range(len(eval_dataset)):\n",
        "    print(\"Question\", i ,\":\",eval_dataset[i][\"question\"])\n",
        "    print(\"Response\",i,\":\",eval_dataset[i][\"response\"])\n",
        "    rouge_scores = scorer.score(generated_responses[i], eval_dataset[i][\"response\"])\n",
        "    print(\"Generated Response: \",generated_responses[i])\n",
        "    print(\"ROUGE scores:\", rouge_scores)\n",
        "    print(\"-----------------------------------------\")\n",
        "# Print the ROUGE scores\n",
        "print(rouge_scores)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c1e220454374430850f0c44f321d737",
            "9298a6617c4b4f1bb6b142ef348d38b8",
            "60ff6fe19573494bb1f7e4fdaf93792f",
            "78541a7571dd46838a8435765a849432",
            "fc3fa75420b7444f8c739586fc3fc535",
            "79856fa86dbe4574b66f4c877d333db2",
            "3c5c3c6f653146a4b7c1ddc4da0a7d6d",
            "3a84c99368cb47dca7371164c2cfd71f",
            "6f3cfa6be88046fd9039e11e4e8a6000",
            "c064c0e4d066439caadb36b8090bf3fa",
            "00ed7539c5944527889aeebf79543613"
          ]
        },
        "id": "6APT3NbYWmsV",
        "outputId": "8b2048ca-807f-4544-b524-a89fef8b005a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Predictions:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c1e220454374430850f0c44f321d737"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 0 : Translate the following sentence to German:\n",
            "This is why we are critical of the proposals put forward by the European Commission and supported by the Council to use the 'Barcelona Process' to undertake, in the framework of this process of association agreements with these countries and of the creation of a 'free trade zone', the liberalisation of services and of agriculture and, generally, the neoliberal guidelines laid down at the WTO Conference in Doha.\n",
            "\n",
            "German:\n",
            "Response 0 : Dies ist der Grund, warum wir die von der Europäischen Kommission vorgelegten und vom Rat unterstützten Vorschläge kritisch sehen, im Rahmen des \"Barcelona-Prozesses\" im Kontext der Assoziierungsabkommen mit diesen Ländern und der Schaffung einer \"Freihandelszone\" die Liberalisierung von Dienstleistungen und Landwirtschaft und im Allgemeinen, die neoliberalen Leitlinien, die auf der WTO-Konferenz in Doha festgelegt wurden, umzusetzen.\n",
            "\n",
            "Step-by-step translation and justification:\n",
            "1. \"This is why we are critical of the proposals\" = \"Dies ist der Grund, warum wir die ... Vorschläge kritisch sehen\"\n",
            "2. \"put forward by the European Commission\" = \"von der Europäischen Kommission vorgelegten\"\n",
            "3. \"and supported by the Council\" = \"und vom Rat unterstützten\"\n",
            "4. \"to use the 'Barcelona Process'\" = \"im Rahmen des 'Barcelona-Prozesses'\"\n",
            "5. \"to undertake, in the framework of this process\" = \"im Kontext der\"\n",
            "6. \"of association agreements with these countries\" = \"Assoziierungsabkommen mit diesen Ländern\"\n",
            "7. \"and of the creation of a 'free trade zone'\" = \"und der Schaffung einer 'Freihandelszone'\"\n",
            "8. \"the liberalisation of services and of agriculture\" = \"die Liberalisierung von Dienstleistungen und Landwirtschaft\"\n",
            "9. \"and, generally\" = \"und im Allgemeinen\"\n",
            "10. \"the neoliberal guidelines laid down at the WTO Conference in Doha\" = \"die neoliberalen Leitlinien, die auf der WTO-Konferenz in Doha festgelegt wurden\"\n",
            "Generated Response:  \n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 1 : For this chain-of-thought reasoning and answer, what was the question?\n",
            "If a mother and her child are hiking the mother is not running through the woods.\n",
            " A: no\n",
            "Response 1 : The question could be: \"Is the mother running through the woods while hiking with her child?\"\n",
            "\n",
            "My thought process to arrive at the answer \"no\" is as follows:\n",
            "\n",
            "1. The given statement says that the mother and her child are hiking.\n",
            "2. It also states that the mother is not running through the woods.\n",
            "3. Hiking typically involves walking at a moderate pace, not running.\n",
            "4. Since the mother is hiking with her child and not running through the woods, the answer to the question is \"no\".\n",
            "Generated Response:  if a mother and her child are hiking\n",
            "ROUGE scores: {'rouge1': Score(precision=0.08045977011494253, recall=0.875, fmeasure=0.14736842105263157), 'rouge2': Score(precision=0.05813953488372093, recall=0.7142857142857143, fmeasure=0.10752688172043011), 'rougeL': Score(precision=0.06896551724137931, recall=0.75, fmeasure=0.12631578947368421)}\n",
            "-----------------------------------------\n",
            "Question 2 : Please answer the following question: What key details about hermann oppenheim  can be extracted from the following bio?  Bio: hermann oppenheim -lrb- 1 january 1858 -- 5 may 1919 -rrb- was one of the leading neurologists in germany . he studied medicine at the universities of berlin , göttingen and bonn . he started his career at the charité-hospital in berlin as an assistant to karl westphal -lrb- 1833 -- 1890 -rrb- . in 1891 oppenheim opened a successful private hospital in berlin . in 1894 , oppenheim was the author of a textbook on nervous diseases titled `` lehrbuch der nervenkrankheiten für Ärzte und studierende '' , a book that soon became a standard in his profession . it was published in several editions and languages , and is considered one of the best textbooks on neurology ever written . he also published significant works on tabes dorsalis , alcoholism , anterior poliomyelitis , syphilis , multiple sclerosis and traumatic neurosis . in the field of physiology , he published articles on metabolism of urea with the aid of nathan zuntz -lrb- 1847 -- 1920 -rrb- . in 1889 he published a treatise on traumatic neuroses that was harshly criticized by eminent physicians that included jean-martin charcot -lrb- 1825 -- 1893 -rrb- and max nonne -lrb- 1861 -- 1959 -rrb- ; the reason being due to oppenheim 's assertion that psychological trauma caused organic changes that perpetuated psychic neuroses . his expertise involving brain disease led directly to the first successful removal of a brain tumor , an operation that was performed by a physician named r. köhler . with surgeon fedor krause -lrb- 1857 -- 1937 -rrb- , he reported the first successful removal of a pineal tumor . he coined the term '' `` dystonia musculorum deformans '' '' for a type of childhood torsion disease he described , a disorder that was later to became known as `` ziehen-oppenheim syndrome '' -lrb- named along with german psychiatrist theodor ziehen 1862 -- 1950 -rrb- . also , another name for amyotonia congenita is `` oppenheim 's disease '' .\n",
            "Answer:\n",
            "Response 2 : Hermann Oppenheim (1 January 1858 – 5 May 1919) was a prominent German neurologist. He studied medicine at the Universities of Berlin, Göttingen, and Bonn and began his career at the Charité-Hospital in Berlin as an assistant to Karl Westphal (1833-1890). In 1891, Oppenheim opened a successful private hospital in Berlin.\n",
            "\n",
            "Oppenheim authored a seminal textbook on nervous diseases in 1894, titled \"Lehrbuch der Nervenkrankheiten für Ärzte und Studierende,\" which became a standard in the field of neurology. The book was published in multiple editions and languages and is considered one of the best neurology textbooks ever written. He contributed significantly to the understanding of various neurological disorders such as tabes dorsalis, alcoholism, anterior poliomyelitis, syphilis, multiple sclerosis, and traumatic neurosis.\n",
            "\n",
            "In the field of physiology, Oppenheim published articles on urea metabolism in collaboration with Nathan Zuntz (1847-1920). His 1889 treatise on traumatic neuroses was met with criticism from renowned physicians Jean-Martin Charcot (1825-1893) and Max Nonne (1861-1959), as he asserted that psychological trauma caused organic changes leading to psychic neuroses.\n",
            "\n",
            "Oppenheim's expertise in brain diseases contributed to the first successful removal of a brain tumor, an operation performed by a physician named R. Köhler. Along with surgeon Fedor Krause (1857-1937), he reported the first successful removal of a pineal tumor.\n",
            "\n",
            "He also coined the term \"dystonia musculorum deformans\" for a childhood torsion disease he described, which later became known as \"Ziehen-Oppenheim syndrome\" (named in conjunction with German psychiatrist Theodor Ziehen, 1862-1950). Additionally, amyotonia congenita is also known as \"Oppenheim's disease\" in his honor.\n",
            "Generated Response:  bio: hermann oppenheim -lrb- 1 january 1858 -- 5 may 1919 -rrb\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0299625468164794, recall=0.7272727272727273, fmeasure=0.05755395683453238), 'rouge2': Score(precision=0.022556390977443608, recall=0.6, fmeasure=0.04347826086956521), 'rougeL': Score(precision=0.0299625468164794, recall=0.7272727272727273, fmeasure=0.05755395683453238)}\n",
            "-----------------------------------------\n",
            "Question 3 : Based on this review, would the user recommend this product? === Review: PIG DESTROYER - Terrifyer-This is some dark, twisted and evil music... yet I can't help but love it! PxDx is a 3 piece grind band that fuses many influences and has a sound so thick that it would consume many 5 pieces in their entirety. The blister guitar work of Scott Hull drives the charge while the rhythmically precise drumming of Brian Harvey holds it together. J.R. Hayes, who has my vote for the craziest lyricist vocalist since Today is the Day's Steve Austin, is in charge on the mic, pushing chaotic extreme vocals to new levels with a voice which is brutal yet still (at times) audible.So what you ask does PxDx sound like you might ask?Devastation! Destruction! Insanity! Chaos! Violence! The Apocalypse!........For extreme metal fans only, this is a treat for the ears.Favorite Songs: Thumbsucker, Gravedancer, Carrion Fairy, Towering Flesh, and The Gentleman.-4.5 StarsIF YOU LIKED, AGREED OR APPRECIATED THIS. PLEASE CLICK YES FOR:\"Was this review helpful?\" Answer:\n",
            "Choose your answer from: i. No ii. Yes\n",
            "Answer:\n",
            "Response 3 : ii. Yes\n",
            "Generated Response:  Please CLICK YES FOR: \"Was this review helpful?\" Answer: Choose your answer from: i. No ii.\n",
            "ROUGE scores: {'rouge1': Score(precision=1.0, recall=0.125, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.5, recall=0.0625, fmeasure=0.1111111111111111)}\n",
            "-----------------------------------------\n",
            "Question 4 : You will be given a definition of a task first, then some input of the task.\n",
            "You are given a sentence in Italian. Your job is to translate the Italian sentence into Arabic.\n",
            "\n",
            "EM: Di solito sono 20 anni, ma la proposta è abbastanza diretta, mi sembra di capire.\n",
            "Output:\n",
            "Response 4 : عادة ما يكون عمري 20 عامًا ، لكن الاقتراح مباشر بما فيه الكفاية ، يبدو لي أنني أفهم.\n",
            "\n",
            "In this task, the definition is to translate an Italian sentence into Arabic. Given the input sentence in Italian, I used the definitions for each word and knowledge of Arabic grammar rules to convert the input into an Arabic sentence. I followed the same order and structure as the original Italian sentence, making sure the translated sentence maintains the same meaning and readability.\n",
            "Generated Response:  EM: Di solito sono 20 anni, ma la proposta è abbastanza diretta, mi sembra\n",
            "ROUGE scores: {'rouge1': Score(precision=0.015625, recall=0.07692307692307693, fmeasure=0.025974025974025976), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.015625, recall=0.07692307692307693, fmeasure=0.025974025974025976)}\n",
            "-----------------------------------------\n",
            "Question 5 : Here is a dialogue:\n",
            "Sally: Hi sorry but I can't make it on Monday - my mum has been taken into hospital.\n",
            "Anna: Oh no! What happened?\n",
            "Sally: She collapsed at home - they are just doing checks at the moment, but don't want to leave her.\n",
            "Anna: Of course! Hope she is feeling better soon.\n",
            "Sally: Yes, me too\n",
            "Anna: Are you still coming in December?\n",
            "Sally: Yes, hopefully - we can do something Christmassy.\n",
            "Anna: Yes that would be good. Say \"hi\" to your mum for me.\n",
            "Sally: Thanks I will. Will let you know how she gets on\n",
            "Anna: Thanks xxx\n",
            "\n",
            "What were they talking about?\n",
            "Response 5 : Sally and Anna were talking about how Sally can't make it to a plan they had on Monday because her mom had to go to the hospital. Sally's mom collapsed at home, so she wants to stay with her. They hope she feels better soon. Then they discuss if Sally will still be able to visit in December, and they want to do something Christmassy together. Anna asks Sally to say \"hi\" to her mom and Sally promises to update Anna about her mom's condition.\n",
            "Generated Response:  Anna: Oh no! What happened? Sally: She collapsed at home - they are just doing checks at the moment, but don't\n",
            "ROUGE scores: {'rouge1': Score(precision=0.11363636363636363, recall=0.47619047619047616, fmeasure=0.18348623853211007), 'rouge2': Score(precision=0.022988505747126436, recall=0.1, fmeasure=0.037383177570093455), 'rougeL': Score(precision=0.07954545454545454, recall=0.3333333333333333, fmeasure=0.12844036697247707)}\n",
            "-----------------------------------------\n",
            "Question 6 : Who played Cardinal Wolsey in the 1971 film ‘Carry On Henry’?\n",
            "Response 6 : In the 1971 funny movie called \"Carry On Henry,\" there's a character called Cardinal Wolsey. A talented actor named Terry Scott played this role. He dressed up like a Cardinal and acted so people could laugh and have a good time while watching the movie.\n",
            "Generated Response:  Richard On Henry\n",
            "ROUGE scores: {'rouge1': Score(precision=0.043478260869565216, recall=0.6666666666666666, fmeasure=0.08163265306122448), 'rouge2': Score(precision=0.022222222222222223, recall=0.5, fmeasure=0.0425531914893617), 'rougeL': Score(precision=0.043478260869565216, recall=0.6666666666666666, fmeasure=0.08163265306122448)}\n",
            "-----------------------------------------\n",
            "Question 7 : Data: Bolt (comicsCharacter), creator, Ernie Colón\n",
            "\n",
            "Sentence about the following data: \n",
            "Response 7 : Bolt, a comics character created by the talented artist Ernie Colón, has captivated the hearts of fans worldwide.\n",
            "\n",
            "Step-by-step justification:\n",
            "1. Identify the main subject: In this case, the main subject is \"Bolt,\" a comics character.\n",
            "2. Identify the secondary subject: The secondary subject is the creator, \"Ernie Colón.\"\n",
            "3. Form the initial sentence structure: [Bolt] is a [comics character] created by [Ernie Colón].\n",
            "4. Enhance the sentence with adjectives or additional information: To make the sentence more interesting, we can add adjectives like \"talented\" to describe the creator, and phrases such as \"captivated the hearts of\" to show the impact the character has had on fans.\n",
            "5. Finalize the sentence: The final sentence is, \"Bolt, a comics character created by the talented artist Ernie Colón, has captivated the hearts of fans worldwide.\"\n",
            "Generated Response:  Ernie Colón Sentence\n",
            "ROUGE scores: {'rouge1': Score(precision=0.02877697841726619, recall=1.0, fmeasure=0.055944055944055944), 'rouge2': Score(precision=0.014492753623188406, recall=0.6666666666666666, fmeasure=0.028368794326241138), 'rougeL': Score(precision=0.02877697841726619, recall=1.0, fmeasure=0.055944055944055944)}\n",
            "-----------------------------------------\n",
            "Question 8 : Q: Lee jumped back into the River after he got his beer.  Given the context: What does Lee need to do before this?\n",
            "The answer is:\n",
            "Response 8 : Before jumping back into the river, Lee needs to get out of the river, go to a place where he can obtain his beer (either a store, bar, or cooler), and then grab his beer.\n",
            "Generated Response:  The answer is:\n",
            "ROUGE scores: {'rouge1': Score(precision=0.02857142857142857, recall=0.3333333333333333, fmeasure=0.05263157894736842), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.02857142857142857, recall=0.3333333333333333, fmeasure=0.05263157894736842)}\n",
            "-----------------------------------------\n",
            "Question 9 : Даже при правильной их конфигурации опытный и настойчивый хакер способен перехватить эти данные.\n",
            "\n",
            "Translate to English\n",
            "Response 9 : \"Even with their correct configuration, an experienced and persistent hacker can intercept this data.\"\n",
            "\n",
            "Although modern security technologies and measures offer robust protection against digital threats, it is important to understand that no system can be completely invulnerable to attacks by skilled and determined hackers. In this statement, the author acknowledges this fact by stating that despite having the correct configuration for protecting data, experienced and persistent hackers may still find a way to intercept this information.\n",
            "\n",
            "This emphasizes the importance of continuous improvement in security practices, staying up-to-date with the latest advancements in the field, and being vigilant about potential security breaches. It is crucial for individuals and organizations dealing with sensitive information to be proactive in maintaining their data security by investing in the latest tools, conducting regular security audits, and training their staff to follow best practices.\n",
            "\n",
            "Additionally, one should consider implementing multiple layers of security (also known as a defense-in-depth strategy) that minimize the potential for a single point of failure and improve overall resilience against cyber attacks. This would include utilizing a combination of encryption, firewalls, intrusion detection systems, and other security measures to make it harder for attackers to gain unauthorized access to data and systems.\n",
            "\n",
            "Moreover, it is essential for organizations to have a well-defined incident response plan in place to efficiently and effectively deal with any security breaches that may occur. This plan would typically include steps such as identification, containment, eradication, recovery, and follow-up to ensure minimal impact on business operations and to prevent future incidents.\n",
            "\n",
            "In summary, it is crucial to recognize that even with the right security measures in place, experienced and persistent hackers can still potentially intercept data. Therefore, it is necessary for individuals and organizations to continuously update their security measures, implement multi-layered defenses, and be prepared to respond effectively to security incidents.\n",
            "Generated Response:  Translate to English English\n",
            "ROUGE scores: {'rouge1': Score(precision=0.003194888178913738, recall=0.25, fmeasure=0.006309148264984227), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.003194888178913738, recall=0.25, fmeasure=0.006309148264984227)}\n",
            "-----------------------------------------\n",
            "Question 10 : Read the following article and answer the question by choosing from the options.\n",
            "\n",
            "Of course , she is having pain but that is to be expected . It 's her right hand so she will be needing help with the basics and her insulin shots but I think we ' ve got that worked out between her kids and me . Her brother and sister - in - law left to go back to Indy yesterday . So , now we have peace and quiet again .\n",
            "\n",
            "What type of disease might the female subject have ?\n",
            "Choose your answer from:\n",
            "a). She has cancer .;\n",
            "b). She has diabetes .;\n",
            "c). None of the above choices .;\n",
            "d). She has chlamydia .;...A:\n",
            "Response 10 : b). She has diabetes.\n",
            "Generated Response:  She has cancer.; b). She has diabetes.; c). None of the above choices\n",
            "ROUGE scores: {'rouge1': Score(precision=1.0, recall=0.3076923076923077, fmeasure=0.47058823529411764), 'rouge2': Score(precision=1.0, recall=0.25, fmeasure=0.4), 'rougeL': Score(precision=1.0, recall=0.3076923076923077, fmeasure=0.47058823529411764)}\n",
            "-----------------------------------------\n",
            "Question 11 : Question: What type of details about james g. mitchell  can be gathered from the following bio?  Bio: james george `` jim '' mitchell -lrb- born 25 april 1943 -rrb- is a canadian computer scientist . he has worked on programming language design and implementation -lrb- fortran , mesa , euclid , c++ , java -rrb- , interactive programming systems , dynamic interpretation and compilation , document preparation systems , user interface design , distributed transactional file systems , and distributed , object-oriented operating systems . he has also worked on the design of hardware for computer graphics , high-level language execution , and audio input output .\n",
            "Answer:\n",
            "Response 11 : The bio provides several significant details about James G. Mitchell.\n",
            "\n",
            "1. Personal Information: James G. Mitchell, also referred to as \"Jim,\" was born on April 25, 1943. He is a Canadian, indicating his nationality. \n",
            "\n",
            "2. Career Field: Mitchell is a computer scientist, implying that his expertise lies in computing technology. It's an extensive field that envelops numerous areas, such as programming, operating systems, computer graphics, and more. His career has been centered around the innovation and evolution of this technology.\n",
            "\n",
            "3. Specific Expertise: He has worked on programming language design and implementation. The bio particularly mentions FORTRAN, Mesa, Euclid, C++, and Java. These are all coding languages that have different applications, suggesting a wide-ranging skill set and in-depth knowledge of programming.\n",
            "\n",
            "4. Interactive Programming Systems: The mention of interactive programming systems indicates his work with interfaces that allow direct communication between the user and the computer program, like developing tools or programs that provide immediate responses to user inputs.\n",
            "\n",
            "5. Dynamic Interpretation and Compilation: Mitchell also has knowledge of dynamic interpretation and compilation. This signifies his comprehension of how programs are executed, which is important for optimizing performance and ensuring that software runs correctly.\n",
            "\n",
            "6. Document Preparation Systems: His work on document preparation systems denotes his involvement in developing software for generating different types of documentation, which can include manuals, guides, and tutorials, among others.\n",
            "\n",
            "7. User Interface Design: He has experience in user interface design, the process of making interfaces in software or computerized devices with a focus on looks or style and on usability and efficient interaction and operation.\n",
            "\n",
            "8. Distributed Transactional File Systems: His expertise in distributed transactional file systems suggests that he understands distributed systems that manage file storage and implementation across multiple sites, nodes, or environments and can ensure that transactions are processed correctly and efficiently.\n",
            "\n",
            "9. Object-Oriented Operating Systems: His work on distributed, object-oriented operating systems shows his acumen for building and maintaining systems that manage computer resources and provide various services.\n",
            "\n",
            "10. Hardware Design: Mitchell's work in the design of hardware for computer graphics, high-level language execution, and audio input/output indicates his range of competences, from developing hardware that accelerates graphic processing, working on hardware that efficiently executes high-level programming language, and creating technology for better sound processing.\n",
            "\n",
            "Overall, the details in the bio depict James G. Mitchell as a well-rounded computer scientist with expertise in various areas of computing technology, from programming languages to hardware design.\n",
            "Generated Response:  Answer:\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
            "-----------------------------------------\n",
            "Question 12 : I can use this background: Greenhouse Effect: The solar energy reaching the surface of the Earth is concentrated in short wavelengths, which can easily penetrate the greenhouse gases, such as carbon dioxide and methane. The Earth, however, is cooler than the sun and it radiates its heat in the form of energy in the far infrared range. These longer wavelengths are partially absorbed by the greenhouse gases and some of the solar heat is returned to Earth. At a certain temperature these processes are in equilibrium and the surface temperature of the Earth is stable. However, if more greenhouse gases are put in the atmosphere the amount of trapped terrestrial radiation increases, leading to an increase in global temperature.  Now, I have a new situation: David was visiting the natural history museum. He noticed two charts that showed the climatic conditions of the earth in two time periods, time A and time B. Time A showed earth's overall climate in the sixteenth century when there were less greenhouse gases in the atmosphere. And time B showed earth's overall climate in the present century with more greenhouse gases in the atmosphere. David found some interesting differences between these two charts.  Answer this question please: Which period would see more trapped terrestrial radiation, time A or time B?\n",
            "Answer:\n",
            "Response 12 : Time B would see more trapped terrestrial radiation. This is because the present century (Time B) has a higher concentration of greenhouse gases in the atmosphere as compared to the sixteenth century (Time A). As a result, more terrestrial radiation is being absorbed and retained by the greenhouse gases during Time B, leading to an overall increase in global temperatures due to the enhanced greenhouse effect.\n",
            "Generated Response:  The solar energy reaching the surface\n",
            "ROUGE scores: {'rouge1': Score(precision=0.030303030303030304, recall=0.3333333333333333, fmeasure=0.05555555555555555), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.030303030303030304, recall=0.3333333333333333, fmeasure=0.05555555555555555)}\n",
            "-----------------------------------------\n",
            "Question 13 : Choose the next sentence.NEW DELHI, India (CNN) -- India's child-rights watchdog has sought a report from police investigating allegations by a tabloid that the father of a \"Slumdog Millionaire\" child star tried to sell her to an undercover reporter, the watchdog's leader told CNN. Rubina Ali has backed her father over newspaper allegations he offered her to an undercover reporter. \"We have sought a report from them and will take a decision after seeing it,\" said Shantha Sinha, who heads the National Commission for Protection of Child Rights. Meanwhile, authorities in Mumbai have recorded the statements of Rafiq Qureshi; his \"Slumdog\" daughter, Rubina Ali; and his former wife, Khurshida Begum, senior police inspector Prakash Salunke told CNN.\n",
            "\n",
            "\n",
            "\n",
            "OPTIONS:\n",
            "- CNN's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Father's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- India's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Indian's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Khurshida Begum's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Mumbai's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- NEW DELHI's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- National Commission for Protection of Child Rights's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Prakash Salunke's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Rafiq Qureshi's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Rubina Ali's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Shantha Sinha's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Slumdog's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- Slumdog Millionaire's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "- UK's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "\n",
            "\n",
            "Response 13 : - Rafiq Qureshi's former wife, whom he divorced several years ago, endorsed the allegations leveled against him by the British newspaper.\n",
            "\n",
            "All the other options are wrong because they mention entities (like CNN, India, Mumbai, etc.) that cannot have a former wife. The correct answer is about Rafiq Qureshi, the father of the \"Slumdog Millionaire\" child star, because the question is about him and the allegations against him.\n",
            "Generated Response:  \n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 14 : Question: what district is sacramento? I found the following answer on Google: Sacramento grew quickly thanks to the protection of Sutter's Fort , which was established by Sutter in 1839. Is that a correct answer? Yes or no.\n",
            "The answer to this question is:\n",
            "Response 14 : No, the information you found does not answer your question about the district of Sacramento. \n",
            "\n",
            "Sacramento is both the capital of California and the county seat of Sacramento County. It is located in California's 6th congressional district, represented by Congresswoman Doris Matsui in the U.S. House of Representatives. Additionally, it is part of California's 6th State Assembly district, represented by Assemblymember Kevin McCarty, and California's 6th State Senate district, represented by Senator Richard Pan. \n",
            "\n",
            "The information you found relates to the history of how Sacramento grew rapidly due to the protection provided by Sutter's Fort, which was established by John Sutter in 1839. Although this is an interesting historical fact, it does not pertain directly to the district of Sacramento.\n",
            "Generated Response:  Sutter's Fort\n",
            "ROUGE scores: {'rouge1': Score(precision=0.024, recall=1.0, fmeasure=0.046875), 'rouge2': Score(precision=0.016129032258064516, recall=1.0, fmeasure=0.031746031746031744), 'rougeL': Score(precision=0.024, recall=1.0, fmeasure=0.046875)}\n",
            "-----------------------------------------\n",
            "Question 15 : Большинство квартир расположены в местах с высокоразвитой инфраструктурой, куда включены магазины различной направленности, небольшие, уютные кафе - все просто создано для полноценной счастливой жизни!\n",
            "\n",
            "Could you please translate this to English?\n",
            "Response 15 : Most apartments are located in places with highly developed infrastructure, which includes different kinds of shops, small cozy cafes - everything is just made for a full, happy life!\n",
            "Generated Response:  олноенно састливо ини\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0, recall=0, fmeasure=0)}\n",
            "-----------------------------------------\n",
            "Question 16 : Possible answers:\n",
            " 1). cuts the parchment paper in half and cuts pieces to carve a face into a face and put a smiley face on the face.\n",
            " 2). fills the cup with water then serves it with a spoon.\n",
            " 3). weld paper in two and starts to wrap the cake with the paper in a strainer to hold onto, ring the cookie up on the baking sheet.\n",
            " 4). brushes on oil to the parchment paper then sets it into a cooking tin.Choose from options above and answer: What most naturally follows?\n",
            "\n",
            "A person measures and cuts parchment paper using a cooking tin as a guide. The person measures ingredients in a glass bowl then adds them to an electric mixer to blend. the person\n",
            "Answer:\n",
            "Response 16 : 4). brushes on oil to the parchment paper then sets it into a cooking tin.\n",
            "\n",
            "Explanation: The person has already measured and prepared the parchment paper and ingredients. The most logical next step would be to prepare the parchment paper for baking by brushing it with oil and setting it into the cooking tin. \n",
            "\n",
            "Option 1 is wrong because it talks about carving a face, which doesn't relate to baking or parchment paper.\n",
            "Option 2 is incorrect because it involves serving water with a spoon, which doesn't follow the situation described.\n",
            "Option 3 is not suitable because it involves wrapping a cake in paper, while the situation describes preparing for baking, not handling an already baked cake.\n",
            "Generated Response:  fills the cup with water then serves it with a spoon. 3). weld paper in two and starts to wrap the cake with the paper\n",
            "ROUGE scores: {'rouge1': Score(precision=0.15966386554621848, recall=0.76, fmeasure=0.26388888888888884), 'rouge2': Score(precision=0.025423728813559324, recall=0.125, fmeasure=0.04225352112676057), 'rougeL': Score(precision=0.09243697478991597, recall=0.44, fmeasure=0.15277777777777776)}\n",
            "-----------------------------------------\n",
            "Question 17 : Please answer the following question: What is the answer for the question: What gender of the Perijá tapaculo have brown on the back of their legs and whitish on the front? from the following article ?  The Perijá tapaculo is a small bird, 10 to 12 cm (3.9 to 4.7 in) in length with an average mass of 17 to 18 grams (around 0.6 oz). The bill averages 6.8 millimetres (0.27 inches) long, 2.9 mm (0.11 in) wide, and 3.5 mm (0.14 in) high. The legs are about 21 mm (0.83 in) long. The Perijá tapaculo's tarsus averages 21.1 millimetres (0.83 in) long. The wings measure 57.4 mm (2.26 in) on average and the tail is about 40 mm (1.6 in) long with between 8 and 12 rectrices.The forehead, lores, crown, mantle, and scapular area are a neutral grey colour. There is a brown spot on the nape. The top of the tail is brown, and the bottom is faintly striped brown. The bird's back and rump are striped brown-sepia, and the throat, breast, and belly are grey-white. Its lower belly and flanks are tawny. The iris is dark brown. Male specimens are distinguished by having less sharp brown spots on their napes, and the bottom of their breasts are mixed with a pale buff colour. The legs are brown on the back and whitish on the front. Young birds have a yellowish appearance with striped brown flanks.The bird's plumage colouration is most similar to the pale-bellied tapaculo. The S. g. morenoi subspecies of the pale-bellied tapaculo can be differentiated from the Perijá tapaculo by its entirely brown back and nape, and its different calls. Juveniles of this subspecies have a dull ventral plumage, while the Perijá tapaculo has a more yellow plumage. It also resembles the Caracas tapaculo but has a duller ventral colour.\n",
            "A:\n",
            "Response 17 : After reading the article, I can determine that the answer to the question \"What gender of the Perijá tapaculo have brown on the back of their legs and whitish on the front?\" is male specimens. The relevant information can be found in the following sentence: \"Male specimens are distinguished by having less sharp brown spots on their napes, and the bottom of their breasts are mixed with a pale buff colour. The legs are brown on the back and whitish on the front.\"\n",
            "Generated Response:  Young birds have a yellowish appearance with striped brown flanks.\n",
            "ROUGE scores: {'rouge1': Score(precision=0.04819277108433735, recall=0.4, fmeasure=0.08602150537634409), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.03614457831325301, recall=0.3, fmeasure=0.06451612903225806)}\n",
            "-----------------------------------------\n",
            "Question 18 : I have a new situation: Deep lake presents geothermal activity, while Shallow lake has mixing of saline water with fresh water. Deep lake had more instances of divers having problems because of thermoclines.  But I can use this background: Thermoclines, or sudden changes in temperature. Where the air temperature is higher than the water temperature, shallow water may be warmed by the air and the sunlight but deeper water remains cold resulting in a lowering of temperature as the diver descends. This temperature change may be concentrated over a small vertical interval, when it is called a thermocline.[3][4] Where cold, fresh water enters a warmer sea the fresh water may float over the denser saline water, so the temperature rises as the diver descends.[3] In lakes exposed to geothermal activity, the temperature of the deeper water may be warmer than the surface water. This will usually lead to convection currents.[3]  What is an answer for this question: Which lake has warmer water the bottom?\n",
            "A:\n",
            "Response 18 : Based on the given information and background, Deep Lake has warmer water at the bottom due to its geothermal activity. \n",
            "\n",
            "In the context of the two lakes, Deep Lake exhibits geothermal activity, which means it is influenced by the Earth's internal heat. This heat rises towards the surface and, in the case of Deep Lake, the temperature of the deeper water may be warmer than the surface water. This causes convection currents, as the warmer, less dense water rises and the cooler, denser water sinks, creating a continuous cycle. \n",
            "\n",
            "On the other hand, Shallow Lake has a mix of saline water with fresh water. While temperature variations can occur in this lake due to the different densities and solubilities of saline and fresh water, it is not mentioned that Shallow Lake has any source of geothermal activity like Deep Lake. This means that its deep water's temperatures will be influenced more by the surrounding environmental factors, and not by heat from within the Earth, which is the case in Deep Lake.\n",
            "\n",
            "In conclusion, Deep Lake has warmer water at the bottom due to the geothermal activity it experiences. This heat, originating from the Earth's interior, progressively warms the deeper waters of the lake and creates distinct thermoclines that divers experience when they descend through the water column. Meanwhile, Shallow Lake's water temperature is more influenced by the mixing of saline and fresh water, without a specific indication of geothermal activity making its deep water warmer compared to Deep Lake.\n",
            "Generated Response:  Deep lake\n",
            "ROUGE scores: {'rouge1': Score(precision=0.007905138339920948, recall=1.0, fmeasure=0.01568627450980392), 'rouge2': Score(precision=0.003968253968253968, recall=1.0, fmeasure=0.007905138339920948), 'rougeL': Score(precision=0.007905138339920948, recall=1.0, fmeasure=0.01568627450980392)}\n",
            "-----------------------------------------\n",
            "Question 19 : Write an article that answers the following question: Who threw the longest touchdown pass of the game?\n",
            "Response 19 : Title: Reliving the Longest Touchdown Pass of the Game: A Moment of Glory\n",
            "\n",
            "Introduction\n",
            "\n",
            "In football, the touchdown pass is one of the most crucial and game-defining plays. Spectators' hearts race when a long touchdown pass is executed with precision and grace, leaving fans in awe and opponents in disbelief. In this article, we will uncover the player responsible for the longest touchdown pass of the game, describe the scenarious and events that led to the throw, and the significance of this play both in the context of the game and the player's career.\n",
            "\n",
            "Step 1: Research\n",
            "\n",
            "To answer the question and determine who threw the longest touchdown pass of the game, I will conduct thorough research using reliable sports websites, news articles, and real-time updates. I will gather accurate data about every touchdown pass that occurred in the game, compare the lengths and timing, and identify the longest throw.\n",
            "\n",
            "Step 2: Identify the key players and plays\n",
            "\n",
            "Once the research is complete, I will have identified the quarterback responsible for the longest touchdown pass of the game along with the recipient. I will also highlight other notable plays during the game, particularly those that contributed to the magnitude of the subsequent pass.\n",
            "\n",
            "Step 3: Describe the scenario and events leading to the throw\n",
            "\n",
            "In detailing the longest touchdown pass, I will include the timing, distance, and field position. Furthermore, I will discuss the strategies and decisions of both the offensive and defensive teams leading to that specific play. This information will give readers a comprehensive understanding of the factors at play before and during the game-defining throw.\n",
            "\n",
            "Step 4: Explain the significance of the longest touchdown pass\n",
            "\n",
            "In this section, I will discuss the implications of the longest touchdown pass on the outcome of the game, specifically how it impacted the teams' scores, morale, and strategy. Additionally, I will explore the play's influence on the players' career trajectories, including that of the quarterback, recipient, and other key participants involved in the play.\n",
            "\n",
            "Conclusion\n",
            "\n",
            "By recounting the exceptional athletic prowess that produced the longest touchdown pass of the game, this article aims to give readers an immersive experience of the game's exhilarating moments. Players' careers and legacies will be assessed based on the plays or contributions made towards the touchdown, providing an insider viewpoint on the forces at play during auspicious match-ups. Football fans and enthusiasts will relish the opportunity to delve into the intricate strategy, precise execution, and sheer athleticism that comes together in the unmatched spectacle of the sport's longest touchdown pass.\n",
            "Generated Response:  Who\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0023148148148148147, recall=1.0, fmeasure=0.0046189376443418004), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0023148148148148147, recall=1.0, fmeasure=0.0046189376443418004)}\n",
            "-----------------------------------------\n",
            "{'rouge1': Score(precision=0.0023148148148148147, recall=1.0, fmeasure=0.0046189376443418004), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0023148148148148147, recall=1.0, fmeasure=0.0046189376443418004)}\n"
          ]
        }
      ]
    }
  ]
}